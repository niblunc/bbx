{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from shutil import rmtree\n",
    "from subprocess import check_output\n",
    "import argparse\n",
    "import subprocess\n",
    "from multiprocessing import Pool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Betaseries for BBx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variable paths \n",
    "\n",
    "ses_id='ses-1'\n",
    "study_path='/projects/niblab/experiments/bbx'\n",
    "input_dir = '/projects/niblab/experiments/bbx/data'    \n",
    "deriv_dir= os.path.join(input_dir, 'preprocessed')\n",
    "design_file='/projects/niblab/experiments/bbx/data/code/beta_design.fsf'\n",
    "ev_path='/projects/niblab/experiments/bbx/data/onsets/trials'\n",
    "preprocess_folder= os.path.join(input_dir, 'preprocessed')\n",
    "\n",
    "subject_folders=sorted(glob.glob('/projects/niblab/experiments/bbx/data/preprocessed/sub-*'))\n",
    "# get subject ids\n",
    "subject_ids=[x.split('/')[-2] for x in glob.glob('/projects/niblab/experiments/bbx/data/preprocessed/sub-*/ses-1')]\n",
    "\n",
    "\n",
    "\n",
    "data_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunks(l,n):\n",
    "    return [l[i:i+n] for i in range(0, len(l), n)]\n",
    "\n",
    "def run_multiprocess(chunks, function, poolsize=2):\n",
    "    with Pool(poolsize) as p:\n",
    "        return_data=p.map(function, chunks)\n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Functions \n",
    "\n",
    "\n",
    "# feat1 fsf file creation \n",
    "def make_file(sub_id, ses_id, trial_id, output_dir, task,data_dict, design_file, newkey, run):\n",
    "    \n",
    "    with open(design_file,'r') as infile:\n",
    "        tempfsf=infile.read()\n",
    "        #\n",
    "        if not os.path.exists(os.path.join(output_dir, \"design_files\")):\n",
    "            os.makedirs(os.path.join(output_dir, \"design_files\"))\n",
    "        #print(output_dir)\n",
    "        \n",
    "        \n",
    "        task_name=trial_id.replace(sub_id+'_',\"\")\n",
    "        design_fileout = os.path.join(output_dir, \"design_files/%s_%s_%s_beta1.fsf\"%(sub_id, ses_id, task_name))\n",
    "        out_param = data_dict[sub_id][newkey][\"TRIALS\"][\"TRIAL%s\"%trial_id][\"OUTPUT\"]\n",
    "        func_param = data_dict[sub_id][newkey][\"FUNCRUN\"]\n",
    "        confound_param = data_dict[sub_id][newkey][\"CONFOUND\"]\n",
    "        trial_param = data_dict[sub_id][newkey][\"TRIALS\"][\"TRIAL%s\"%trial_id][\"TRIAL\"]\n",
    "        nuis_param = data_dict[sub_id][newkey][\"TRIALS\"][\"TRIAL%s\"%trial_id][\"NUIS\"]\n",
    "        #print(design_fileout)\n",
    "        tempfsf = tempfsf.replace(\"OUTPUT\", out_param)\n",
    "        tempfsf = tempfsf.replace(\"FUNCRUN\", func_param) \n",
    "        tempfsf = tempfsf.replace(\"CONFOUND\", confound_param)\n",
    "        tempfsf = tempfsf.replace(\"TRIAL\", trial_param)\n",
    "        tempfsf = tempfsf.replace(\"NUIS\", nuis_param)\n",
    "\n",
    "        for i in range(6):\n",
    "            moco = data_dict[sub_id][newkey][\"MOCO%i\"%i]\n",
    "            tempfsf = tempfsf.replace(\"MOCO%i\"%i, moco)\n",
    "        #print(tempfsf)\n",
    "        try:\n",
    "            with open(design_fileout,'w') as outfile:\n",
    "                outfile.write(tempfsf)\n",
    "            outfile.close()\n",
    "        except:\n",
    "            print(\"BAD SUBJECT \", sub_id)\n",
    "        infile.close()\n",
    "        \n",
    "        \n",
    "def create_fsf(input_dir, deriv_dir, ses_id, design_file, ev_path):\n",
    "    \n",
    "    ses_id=ses_id\n",
    "    preproc_folder = deriv_dir\n",
    "    \n",
    "    data_dict={}\n",
    "    \n",
    "    \n",
    "    # start loop -- looping through subjects\n",
    "    subject_list = glob.glob(os.path.join(preproc_folder, 'sub-*/%s'%ses_id))\n",
    "    for sub_path in subject_list:\n",
    "        # set variables from file path\n",
    "        sub_id=sub_path.split(\"/\")[-2]\n",
    "        # add subject id to dictionary\n",
    "        if sub_id not in data_dict:\n",
    "            data_dict[sub_id] = {}\n",
    "            \n",
    "            \n",
    "            \n",
    "        # get functional task files \n",
    "        # this section may be unique to an individual study,\n",
    "        # make sure the extraction applies correctly. \n",
    "        \n",
    "        # get list of nifti functional files of current subject\n",
    "        # we use the `_preproc_bold_brain.nii.gz task files, these are preprocessed niftis\n",
    "        functional_tasks = glob.glob(os.path.join(sub_path, 'func/*preproc_bold_brain.nii.gz'))\n",
    "                \n",
    "        \n",
    "        for functional in functional_tasks: \n",
    "            # set specific variables from filename\n",
    "            task=functional.split(\"/\")[-1].split(\"_\")[2].split(\"-\")[1]\n",
    "            if 'resting' in task:\n",
    "                run=\"\"\n",
    "                pass\n",
    "            else:\n",
    "                run=functional.split(\"/\")[-1].split(\"_\")[3]\n",
    "\n",
    "                \n",
    "                #print(task,run)\n",
    "\n",
    "                analysis_folder=os.path.join(deriv_dir, '%s/%s'%(sub_id,ses_id), \"analysis\")\n",
    "                #print(analysis_folder)\n",
    "                output_dir = os.path.join(analysis_folder, 'beta/task-%s_%s'%(task,run))\n",
    "                #print('[INFO] output directory: ', output_dir)\n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "\n",
    "                # set confound file\n",
    "                #sub-001_ses-1_task-training_run-1_space-MNI152NLin2009cAsym_desc-preproc_confound.txt\n",
    "\n",
    "                if 'resting' in task:\n",
    "                    confound_file = os.path.join(sub_path, \"func/motion_assessment/%s_%s_task-%s_space-MNI152NLin2009cAsym_desc-preproc_confound.txt\"%(sub_id,ses_id, task))\n",
    "                else:\n",
    "                    confound_file = os.path.join(sub_path, \"func/motion_assessment/%s_%s_task-%s_%s_space-MNI152NLin2009cAsym_desc-preproc_confound.txt\"%(sub_id,ses_id, task, run))\n",
    "\n",
    "                #print(confound_file)\n",
    "                #print(run)\n",
    "                if 'run' in run:\n",
    "                    newkey=\"%s_%s\"%(task, run)\n",
    "                else:\n",
    "                    newkey=\"%s\"%task\n",
    "                #print(newkey)\n",
    "                data_dict[sub_id][newkey] = {\n",
    "                    \"TRIALS\" : { },\n",
    "                    \"CONFOUND\" : confound_file,\n",
    "                    \"FUNCRUN\" : functional\n",
    "                  }           \n",
    "\n",
    "                # set motion parameters\n",
    "                # sub-001_ses-1_task-training_run-1_moco0.txt | sub-001_ses-1_task-resting_moco5.txt\n",
    "                for i in range(6):\n",
    "                    if 'run' in run:\n",
    "                        motcor=os.path.join(sub_path, 'func','motion_assessment', 'motion_parameters','%s_%s_task-%s_%s_moco%s.txt'%(sub_id,ses_id, task,run,i))\n",
    "                    else:\n",
    "                        motcor=os.path.join(sub_path, 'func','motion_assessment', 'motion_parameters','%s_%s_task-%s_moco%s.txt'%(sub_id,ses_id, task,i))\n",
    "                    #print(motcor)\n",
    "                    data_dict[sub_id][newkey]['MOCO%i'%i] = motcor\n",
    "\n",
    "                #print(data_dict)\n",
    "\n",
    "                # setup evs(onsets)\n",
    "                trial_evs = sorted(glob.glob(os.path.join(ev_path, '%s_*%s*.txt'%(sub_id, run))))\n",
    "                #print(trial_evs[0])\n",
    "\n",
    "                if not trial_evs:\n",
    "                    pass\n",
    "                else:\n",
    "                    for trial_file in trial_evs:\n",
    "                        _id = sub_id.split(\"-\")[1]\n",
    "                        _id = _id[1:]\n",
    "                        trial_id = trial_file.split(\"/\")[-1].split(\".\")[0]\n",
    "                        #print(trial_id)\n",
    "                        nuis_file = os.path.join(ev_path, '%s.txt'%trial_id.replace('trial', 'nuis'))\n",
    "                        #print(nuis_file)\n",
    "                        \n",
    "                        trial_ext=trial_id.replace(sub_id+'_',\"\")\n",
    "                        fileout = os.path.join(output_dir, \"%s_%s_%s\"%(sub_id, ses_id,trial_ext))\n",
    "                        #print(fileout)\n",
    "                        \n",
    "                        # fill dictionary\n",
    "                        data_dict[sub_id][newkey][\"TRIALS\"][\"TRIAL%s\"%trial_id] = {\"TRIAL\" : trial_file, \"NUIS\": nuis_file, \"OUTPUT\" : fileout}\n",
    "                        \n",
    "                        # write out file\n",
    "                        make_file(sub_id, ses_id, trial_id,output_dir, task, data_dict,design_file, newkey, run)\n",
    "\n",
    "                \n",
    "    return(data_dict) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Feat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] making fsf design files...\n",
      "[INFO] process complete.\n"
     ]
    }
   ],
   "source": [
    "# make feat1 fsf files\n",
    "print('[INFO] making fsf design files...')\n",
    "#data_dict=create_fsf(input_dir, deriv_dir,ses_id,design_file,ev_path)\n",
    "print('[INFO] process complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets view a few of files created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sub-001_ses-1_task-H2O-run-4_nuis1_beta1.fsf',\n",
       " 'sub-001_ses-1_task-H2O-run-4_nuis2_beta1.fsf',\n",
       " 'sub-001_ses-1_task-H2O-run-4_nuis3_beta1.fsf',\n",
       " 'sub-001_ses-1_task-H2O-run-4_nuis4_beta1.fsf',\n",
       " 'sub-001_ses-1_task-H2O-run-4_nuis5_beta1.fsf']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at created files\n",
    "output_path='/projects/niblab/experiments/bbx/data/preprocessed/sub-001/ses-1/analysis/beta/task-training_run-4/design_files'\n",
    "listdir(output_path)[:5]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 64784.000000 fsf files available.\n"
     ]
    }
   ],
   "source": [
    "beta1_files_path='/projects/niblab/experiments/bbx/data/preprocessed/sub-*/ses-1/analysis/beta/task-training_run-*/design_files/*.fsf'\n",
    "print('[INFO] %f fsf files available.'%len(glob.glob(beta1_files_path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/projects/niblab/experiments/bbx/data/preprocessed/sub-001/ses-1/analysis/beta/task-training_run-4/design_files/sub-001_ses-1_task-H2O-run-4_nuis1_beta1.fsf',\n",
       " '/projects/niblab/experiments/bbx/data/preprocessed/sub-001/ses-1/analysis/beta/task-training_run-4/design_files/sub-001_ses-1_task-H2O-run-4_nuis2_beta1.fsf',\n",
       " '/projects/niblab/experiments/bbx/data/preprocessed/sub-001/ses-1/analysis/beta/task-training_run-4/design_files/sub-001_ses-1_task-H2O-run-4_nuis3_beta1.fsf',\n",
       " '/projects/niblab/experiments/bbx/data/preprocessed/sub-001/ses-1/analysis/beta/task-training_run-4/design_files/sub-001_ses-1_task-H2O-run-4_nuis4_beta1.fsf',\n",
       " '/projects/niblab/experiments/bbx/data/preprocessed/sub-001/ses-1/analysis/beta/task-training_run-4/design_files/sub-001_ses-1_task-H2O-run-4_nuis5_beta1.fsf']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view a few of the files\n",
    "glob.glob(beta1_files_path)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit slurm jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'H2O', 'H2Ocue', 'SSB', 'SSBcue', 'USB', 'USBcue', 'rinse'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks=[x.split(\"/\")[-1].split(\"_\")[2].split(\"-\")[1] for x in glob.glob(output_path+\"/*.fsf\")]\n",
    "set(tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def by_file_slurm(subject,ev_task):\n",
    "    fsf_path='/projects/niblab/experiments/bbx/data/preprocessed/{}/ses-1/analysis/beta/task-training_run-*/design_files/*{}*.fsf'.format(subject, ev_task)\n",
    "    fsfs=glob.glob(fsf_path)\n",
    "    for fsf in fsfs:\n",
    "        slurm_cmd = \"sbatch /projects/niblab/experiments/bbx/data/code/beta_by_file.job {}\".format(fsf)\n",
    "        #print('[INFO] submitted: \\n', slurm_cmd)\n",
    "        os.system(slurm_cmd)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] setting up batch jobs.\n",
      "[INFO] submitted batch jobs.\n"
     ]
    }
   ],
   "source": [
    "# run all jobs\n",
    "print('[INFO] setting up batch jobs.')\n",
    "ev_task='USB'\n",
    "for subject in subject_ids[20:60]:    \n",
    "    by_file_slurm(subject, ev_task)\n",
    "    \n",
    "print('[INFO] submitted batch jobs.')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!squeue -u nbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup Data Dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_subjects=[]\n",
    "for folder in subject_folders:\n",
    "    subject_id=folder.split(\"/\")[-1]\n",
    "    tasks=glob.glob(os.path.join(folder,'ses-1/analysis/beta/*'))\n",
    "    #print('[INFO] tasks: ', tasks)\n",
    "    \n",
    "    if not tasks:\n",
    "        bad_subjects.append(subject_id)\n",
    "    else:\n",
    "        if subject_id not in data_dict:\n",
    "            data_dict[subject_id]={}\n",
    "        for task_folder in tasks:\n",
    "            #print(task_folder)\n",
    "            task=task_folder.split(\"/\")[-1].split('.')[0]\n",
    "            #print('[INFO] task: ', task)\n",
    "            if task not in data_dict[subject_id]:\n",
    "                data_dict[subject_id][task]={}\n",
    "            pes=glob.glob(os.path.join(task_folder, '*.feat/stats/pe1.nii.gz'))\n",
    "            data_dict[subject_id][task]=pes\n",
    "       \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dict['sub-001']['task-training_run-1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate PEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword=\"USBcue\"\n",
    "bad_subjects=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_files(subject_list, data_dict=data_dict,keyword=keyword):\n",
    "\n",
    "    for subject_id in subject_list[:2]:\n",
    "        for task in data_dict[subject_id]:\n",
    "            #print('[INFO] task: ', task)\n",
    "            pes=[pe for pe in data_dict[subject_id][task] if keyword in pe]\n",
    "            if \"-\" in keyword:\n",
    "                keyword=keyword.strip('-')\n",
    "                \n",
    "            #print(\"[INFO] PEs: \",pes)\n",
    "            filename=\"%s_%s_%s\"%(subject_id, task, keyword)\n",
    "            outfile = \"/projects/niblab/experiments/bbx/data/betaseries/niftis/%s/%s\"%(keyword, filename)\n",
    "            #print('[INFO] outfile: \\n',outfile)\n",
    "            if not os.path.exists(\"/projects/niblab/experiments/bbx/data/betaseries/niftis/%s\"%keyword):\n",
    "                 os.makedirs(\"/projects/niblab/experiments/bbx/data/betaseries/niftis/%s\"%keyword)\n",
    "                    \n",
    "            if not pes:\n",
    "                bad_subjects.append(subject_id)\n",
    "            else:\n",
    "                pe_str = \" \".join(pes)\n",
    "                fslmerge_cmd=\"/projects/niblab/modules/software/fsl/5.0.10/bin/fslmerge -t %s %s\"%(outfile, pe_str)\n",
    "                #print(\"[INFO] running fsl merge command...\")\n",
    "                #print(fslmerge_cmd, \"\\n\")\n",
    "                os.system(fslmerge_cmd)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] chunksize: 10\n",
      "[INFO] running fslmerge ....\n",
      "fslmerge -t [OUTPUT FILENAME][PE FILES]\n",
      "[INFO] process complete.\n"
     ]
    }
   ],
   "source": [
    "chunksize=10\n",
    "print(\"[INFO] chunksize: {}\".format(chunksize))\n",
    "chunk_list=chunks(subject_ids, chunksize)\n",
    "\n",
    "#print(chunk_list)\n",
    "print(\"[INFO] running fslmerge ....\\nfslmerge -t [OUTPUT FILENAME][PE FILES]\")\n",
    "\n",
    "with Pool(8) as p:\n",
    "    p.map(merge_files, chunk_list)\n",
    "print(\"[INFO] process complete.\")\n",
    "#print('[INFO] find images here: ',concat_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranform_niftis(niftis, keyword=keyword):\n",
    "    reference_nifti='/projects/niblab/parcellations/chocolate_decoding_rois/mni2ace.nii.gz'\n",
    "    reference_mat='/projects/niblab/parcellations/chocolate_decoding_rois/mni2ace.mat'\n",
    "    for nii in niftis:\n",
    "\n",
    "        # setup and run flirt\n",
    "        nii=nii.replace('.nii.gz', '')\n",
    "        out=nii#+'_3mm'\n",
    "        flirt_cmd=\"flirt -in {} -ref {} -init {} -applyxfm -out {}\".format(nii, reference_nifti, reference_mat, out)\n",
    "        #print('[INFO] flirt command: \\n{}'.format(flirt_cmd))\n",
    "        os.system(flirt_cmd)\n",
    "\n",
    "        fslmaths_cmd='fslmaths {} -thr 0.9 {}'.format(out,out)\n",
    "        #print('[INFO] fslmaths command: \\n{}'.format(fslmaths_cmd))\n",
    "        os.system(fslmaths_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] transform functionals to match the mask.\n",
      "[INFO] chunksize: 10\n",
      "[INFO] transformation process complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('[INFO] transform functionals to match the mask.')\n",
    "chunksize=10\n",
    "# grab concatenated (fslmerge) data\n",
    "keyword=\"SSBcue\"\n",
    "fslmerged_files=glob.glob(os.path.join('/projects/niblab/experiments/bbx/data/betaseries/niftis/%s/*.nii.gz'%(keyword)))\n",
    "#fslmerged_files[:4]\n",
    "print(\"[INFO] chunksize: {}\".format(chunksize))\n",
    "chunk_list=chunks(fslmerged_files, chunksize)\n",
    "with Pool(8) as p:\n",
    "    p.map(tranform_niftis, chunk_list)\n",
    "print('[INFO] transformation process complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3274019\r\n"
     ]
    }
   ],
   "source": [
    "!sbatch /projects/niblab/experiments/bbx/data/betaseries/timeseries_roi_pull.job SSB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\r\n",
      "           3274019     batch timeseri   nbytes  R       0:43      1 largemem-0-0\r\n"
     ]
    }
   ],
   "source": [
    "!squeue -j 3274019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
